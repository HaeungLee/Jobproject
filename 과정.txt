1. 셀레니움으로 로그인해서 원하는 페이지 1페이지 크롤링 구현
    1-1. 어려웠던 문제, 팝업창 및 hover시 나타나는 메뉴는 클릭이 어려웠음 -> 해결방법: ActionChains로 hover 후 클릭, Xpath로 지정, script를 강제변경
    1-2. 새창 문제. -> 해결방법: driver.window_handles로 새창을 열고, 해당 창으로 포커스를 이동한 후 크롤링

2. 몇번 시도하니까 막히는 문제 발생 -> 
    셀레니움으로 로그인하고 페이지 넘기고 BeautifulSoup으로 크롤링 방법을 이용해봄. Agent를 최신버전으로 변경해봄. time.sleep()을 여기저기 넣음

3. 전체 데이터 크롤링 구현
    3-1. 크롤링 로직 지속 수정
    3-2. 크롤링한 데이터를 csv및 excel로 저장 -> DataFrame으로 저장해서 데이터 처리가 쉽게, 나중에 시각화 처리에 용이하게 + 백업용
    3-3. 이미지데이터 문자변환 - > ~ing

4. 전처리 및 시각화
    -- views.py 지속 수정
    4-1. 중복데이터 문제 및 NaN값 처리 -> set()으로 중복을 막아봄. 중복데이터처리, 정규화식 사용해봄, 
    4-2. 크롤링한 데이터의 시각화를 위해 matplotlib, seaborn 사용 -> django에서 어떻게 불러오는지 몰라서 검색 해봄
    4-3. 시각화한 데이터를 html로 저장하기 위해 matplotlib의 savefig() 사용
    4-4. 여러가지 써보면서 의미있는 insights 만 남기려고 노력함
    4-5. AI에게 html 초안을 맡기고 html에서 불러오기 해봄
    4-6. 하드코딩된 부분을 하나씩 동적 데이터로 수정
    4-7. views.py에 백업 데이터를 인사이트 페이지에서 불러오는 기능 추가 -> DB와 파일데이터 모두 활용 가능하게
    4-8. UI 지속 수정 --> ~ing
    4-9. 시각화 결과를 사용자에게 더 잘 보여주기 위해 대시보드 기능 추가 계획(고려)

추가 발전 고려사항: 다른 사이트도 크롤링해서 데이터 통합
    1. 잡코리아, 사람인, 인크루트 등 다양한 사이트에서 크롤링해서 데이터 통합 및 인사이트-> 나아가서 국가별, linkedin 등 - 미국, 일본, 중국
    2. 각 사이트의 데이터 형식이 다르기 때문에 통합하는 과정에서 데이터 정제 및 변환 필요 - AI 추천
    3. 각 사이트의 API를 활용하여 데이터 수집 효율성 향상 - AI 추천
    4. 최근 지원자들을 AI 분석으로 선별해서 채용한다는 이야기가 있는데, 로직을 파악해서 지원자에게 추천되게끔 맞춤 조언하는 기능을 추가

5. 발표하려고 하니 아이디 / 비밀번호가 하드코딩 되어 있어서 환경변수로 바꿈

6. 강의실에서 발표하기 위해 requirements.txt작성 - 필요한 라이브러리 추가 및 호환성 검사
    6-1. pip freeze > requirements.txt로 작성
    6-2. 호환성 검사 -> pip check로 확인
    6-3. 필요없는 라이브러리 삭제 및 버전 고정
    6-4. requirements.txt에 필요한 라이브러리 추가

7. ML, DL 복습 및 적용
    7-1. K-means 클러스터링을 통해 비슷한 특성을 가진 채용공고들을 그룹화 -> 비슷한 직무 추천가능
    7-2. 직무 특성 기반으로 Random Forest 모델을 이용해서 급여 범위 예측 -> 급여 정보가 없는 채용공고에 예상 급여 제공 가능
    7-3. 과거 채용 데이터 바탕으로 미래 채용 트렌드 예측 -> ~ing
    7-4. 재미로 예측 합격률을 넣고 싶었지만, 합격자 정보가 부족하고, 낮게 나오면 자신감을 떨어트릴 수 있을 것 같아서 제외함
    7-5. ml_sights.html을 만들어서 ML/DL 결과를 시각화해서 보여줌

8. AI에게 이런 프로그램을 만들었는데 어떻게 생각하냐고 물어봄 -> Django Celery를 사용해서 정기적인 크롤링 스케줄링을 구현하면 좋을 것 같다고 함. 재밌을것 같아서 시도
    8-1. 설치 및 설정 방법을 물어보고 기초 코드를 달라고 해서 기초 코드를 통해 초기버전 완성. 이 후에 모르는 부분은 물어봄
    8-2. 매우 오류가 많이 남 -> AI에게 코드를 맡김 - 오류가 계속남 -> 검색해봄 -> 그래도 안됨 -> 관련 documents를 다 찾아봄
    8-3. 코드 전체를 뜯어고쳐서 어떻게든 실행되게 함. 경로 문제도 복잡하고 잘되는지 의문 -> 강의실에서 실험해보고 지속 개선 예정
    8-4. 정신이 매우 피폐해짐

9. 사전 학습된 LLM을 불러와 customizing, 페이지를 학습시키고 고급 분석 기능 사용할 수 있게 구현해봄
    9-1. 모델을 불러와서 fine-tuning시도 -> 안됨. 자기 전에 돌리고 일어났는데 1epoch가 안돌았음. 컴퓨팅 파워 부족
    9-2. prompt engineering시도 -> 작은 파라미터 모델 때문인지 prompt가 미숙해서 그런건지 모델이 멍청해지는 문제 발생
    9-3. 위 두가지에서 얻은 대안 : LoRA 적용 -> 추가적인 실험을 통해 성능 개선을 시도할 예정

    추가로 구현할 점: Celery 작업에 모델 재학습 태스크 추가
                    새로운 데이터가 충분히 쌓이면 자동으로 모델 업데이트

<느낀점 & 발전 고려사항>
아직 routing 문제가 어려움 - 더 해봐야 함
이미지 데이터 문자변환 숙달 필요 -> OCR을 사용해서 이미지 데이터를 문자로 변환 -> pytesseract 라이브러리 사용, 이미지 전처리 및 OCR 처리
Django Celery 같은 스케줄링 프로그램을 java같은 다른 언어나 flask, scriplit 같은 곳에서 어떻게 처리하는지 알고싶음
초대규모 LLM의 customizing을 어떻게 하는지 알고 싶음 (ex.Perflexity)
데이터가 더 많으면 좋을 것 같음
회사를 컴퓨팅 파워로 구분해 볼듯
피드백을 받고싶음 - 개선점, ML/DL 활용법
Django말고도 로컬 배포방법을 알아내야함